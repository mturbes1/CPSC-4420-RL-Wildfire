#!/usr/bin/env python
# coding: utf-8

# In[15]:


# Import required libraries
import os
import cv2
import numpy as np
import random
from collections import deque
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber


# In[16]:


# Define dataset paths
rgb_frames_dir = "Segmented Frames/RGB"
ir_frames_dir = "Segmented Frames/IR"


# In[17]:


# Define the WildfireEnvironment class
class WildfireEnvironment:
    def __init__(self, rgb_frames_dir, ir_frames_dir, video_subset=None):
        self.rgb_frames = self.load_frames(rgb_frames_dir, video_subset)
        self.ir_frames = self.load_frames(ir_frames_dir, video_subset)
        self.num_frames = len(self.rgb_frames)
        self.current_index = 0

    def load_frames(self, folder, video_subset=None):
        """Load and preprocess frames from a specified subset of videos."""
        frames = []
        for video_folder in sorted(os.listdir(folder)):  # Loop through all video subfolders
            if video_subset and video_folder not in video_subset:
                continue  # Skip videos not in the subset
            video_path = os.path.join(folder, video_folder)
            if os.path.isdir(video_path):  # Ensure it's a directory
                for filename in sorted(os.listdir(video_path)):
                    filepath = os.path.join(video_path, filename)
                    image = cv2.imread(filepath)
                    if image is not None:
                        image = cv2.resize(image, (84, 84))  # Resize to match input size
                        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
                        frames.append(image / 255.0)  # Normalize to [0, 1]
        return np.array(frames)

    def reset(self):
        """Reset the environment to the initial state."""
        self.current_index = 0
        return self.get_state()

    def get_state(self):
        """Get the current state as a combination of RGB and IR frames."""
        if self.current_index >= self.num_frames:
            return None
        rgb_frame = self.rgb_frames[self.current_index]
        ir_frame = self.ir_frames[self.current_index]
        return np.stack([rgb_frame, ir_frame], axis=-1)  # Stack as a 2-channel image

    def step(self, action):
        """
        Perform an action and move to the next state.
        
        Parameters:
            action (int): Predicted fire spread direction (0: no spread, 1: left, 2: right, 3: up, 4: down)
        
        Returns:
            next_state (np.array): The next RGB+IR state.
            reward (float): Reward based on prediction accuracy.
            done (bool): Whether the episode has ended.
        """
        # Move to the next frame
        self.current_index += 1
        done = self.current_index >= self.num_frames  # Check if end of video
    
        if done:
            return None, 0, True  # No next state, zero reward, and done flag
    
        # Get the next state
        next_state = self.get_state()
    
        # Define the actual wildfire behavior
        actual_direction = self.get_actual_fire_direction()
    
        # Compute reward based on prediction accuracy
        if action == actual_direction:
            reward = 1  # Reward for exact match
        elif abs(action - actual_direction) == 1:
            reward = 0.5  # Partial match
        else:
            reward = -1  # Penalty for incorrect prediction

        return next_state, reward, done
    
    def get_actual_fire_direction(self):
        """
        Compute actual wildfire spread direction based on pixel differences between frames.
        
        Returns:
            int: Actual wildfire spread direction (0: no spread, 1: left, 2: right, 3: up, 4: down)
        """
        if self.current_index == 0 or self.current_index >= self.num_frames - 1:
            return 0  # No spread for the first frame or after the last frame
    
        # Get the current and next frames
        current_frame = self.rgb_frames[self.current_index - 1]
        next_frame = self.rgb_frames[self.current_index]
    
        # Compute pixel differences
        diff = next_frame - current_frame  # Frame difference matrix
        height, width = diff.shape
    
        # Split the frame into regions
        upper_region = diff[:height // 2, :]  # Top half for upward spread
        lower_region = diff[height // 2:, :]  # Bottom half for downward spread
        left_region = diff[:, :width // 2]    # Left half for leftward spread
        right_region = diff[:, width // 2:]   # Right half for rightward spread
    
        # Compute summed differences in each region
        total_up = np.sum(upper_region)
        total_down = np.sum(lower_region)
        total_left = np.sum(left_region)
        total_right = np.sum(right_region)
    
        # Threshold-based direction assignment
        threshold_up = 50     # Example thresholds; adjust based on dataset
        threshold_down = -50
        threshold_left = -50
        threshold_right = 50
    
        if total_up > threshold_up:
            return 3  # Upward
        elif total_down < threshold_down:
            return 4  # Downward
        elif total_left < threshold_left:
            return 1  # Left
        elif total_right > threshold_right:
            return 2  # Right
        return 0  # Default: No spread


# In[18]:


# List all video subfolders
all_videos = sorted(os.listdir(rgb_frames_dir))  # Ensure the directories are consistent

# Split into training and testing subsets
train_videos = all_videos[:5]  # Use the first 5 videos for training
test_videos = all_videos[5:]   # Use the remaining 2 videos for testing

# Create environments for training and testing
train_env = WildfireEnvironment(rgb_frames_dir, ir_frames_dir, video_subset=train_videos)
test_env = WildfireEnvironment(rgb_frames_dir, ir_frames_dir, video_subset=test_videos)


# In[19]:


# Define the DQNAgent class
class DQNAgent:
    def __init__(self, state_shape, action_size):
        self.state_shape = state_shape
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # Discount factor
        self.epsilon = 1.0  # Exploration rate
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.learning_rate = 0.001
        self.model = self._build_model()

    def _build_model(self):
        """Build a Convolutional Neural Network (CNN) for Q-value prediction."""
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=self.state_shape),
            Conv2D(64, (3, 3), activation='relu'),
            Flatten(),
            Dense(128, activation='relu'),
            Dense(self.action_size, activation='linear')  # Q-values for each action
        ])
        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=Huber())
        return model

    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay memory."""
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        """Choose action based on epsilon-greedy strategy."""
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        q_values = self.model.predict(state[np.newaxis], verbose=0)
        return np.argmax(q_values[0])

    def replay(self, batch_size):
        """Train the model using randomly sampled experiences."""
        if len(self.memory) < batch_size:
            return
        batch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in batch:
            target = reward
            if not done:
                target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])
            target_f = self.model.predict(state[np.newaxis], verbose=0)
            target_f[0][action] = target
            self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


# In[24]:


# Define the training loop
def train_dqn(train_env, episodes=50, batch_size=32):
    state_shape = (84, 84, 2)  # 84x84 resolution, 2 channels (RGB + IR)
    action_size = 5  # Number of possible actions
    agent = DQNAgent(state_shape, action_size)

    for e in range(episodes):
        state = train_env.reset()
        total_reward = 0
        for time in range(train_env.num_frames):
            action = agent.act(state)
            next_state, reward, done = train_env.step(action)
            total_reward += reward
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            if done:
                print(f"Episode {e+1}/{episodes} - Total Reward: {total_reward}")
                break
        agent.replay(batch_size)

        # Save the model immediately after completing the episode
        if (e + 1) % 10 == 0:  # Align with completed episode number
            agent.model.save(f"dqn_model_episode_{e + 1}.h5")
            print(f"Model saved at episode {e + 1}")

    # Save final model after all episodes
    agent.model.save("dqn_model_episode_final.h5")
    print("Final model saved after training.")
                
    return agent


# In[25]:


# Train the DQN agent on the training set
dqn_agent = train_dqn(train_env, episodes=50, batch_size=32)


# In[20]:


#Testing the agent

# Load the saved model
from tensorflow.keras.models import load_model

# Reload the environment
test_env = WildfireEnvironment(rgb_frames_dir, ir_frames_dir)

# Load the trained agent
trained_model = load_model('dqn_model_episode_final.h5')

# Test the trained agent on the test set
def test_dqn(agent, env, episodes=5):
    total_rewards = []

    for e in range(episodes): # Loop over the number of episodes
        state = env.reset()
        total_reward = 0
        while True:
            action = np.argmax(agent.predict(state[np.newaxis], verbose=0))  # Use greedy policy
            next_state, reward, done = env.step(action)
            total_reward += reward
            state = next_state
            if done:
                if done:
                    print(f"Episode {e + 1}/{episodes} - Total Reward: {total_reward}")
                    total_rewards.append(total_reward)
                    break
    
    print(f"Average Reward over {episodes} test episodes: {np.mean(total_rewards)}")


# In[21]:


# Test the trained model
test_dqn(trained_model, test_env, episodes=5)
